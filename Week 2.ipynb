{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download_file(\"blossom-data-engs\", \"all-us-stocks-tickers-company-info-logos.zip\", \"all-us-stocks-tickers-company-info-logos.zip\")\n",
    "s3.download_file(\"blossom-data-engs\", \"data-scientist-job-market-in-the-us.zip\", \"data-scientist-job-market-in-the-us.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe from csv file\n",
    "# when inferSchema is True spark scans the file once to detect the schema\n",
    "companies = spark.read.csv(\n",
    "            \"companies.csv\", \n",
    "            header=True, inferSchema=True)\n",
    "    \n",
    "alldata = spark.read.csv(\n",
    "            \"alldata.csv\", \n",
    "            header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            industry|         description|             website|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Medical Diagnosti...|Agilent Technolog...|http://www.agilen...|\n",
      "|     Metals & Mining|Alcoa Corp is an ...|http://www.alcoa.com|\n",
      "|    Asset Management|Altaba Inc is an ...|http://www.altaba...|\n",
      "|Health Care Provi...|AAC Holdings Inc ...|http://www.americ...|\n",
      "|                null|The investment se...|                null|\n",
      "|            AADR.png|                null|           NYSE Arca|\n",
      "|            Airlines|American Airlines...|   http://www.aa.com|\n",
      "|    Asset Management|Altisource Asset ...|http://www.altiso...|\n",
      "|    Insurance - Life|Atlantic American...|http://www.atlam.com|\n",
      "|Consulting & Outs...|Aaron's Inc is a ...|http://www.aarons...|\n",
      "|      Semiconductors|Applied Optoelect...|http://www.ao-inc...|\n",
      "|  Building Materials|AAON Inc is a hea...| http://www.aaon.com|\n",
      "|Retail - Apparel ...|Advance Auto Part...|https://shop.adva...|\n",
      "|   Computer Hardware|Apple Inc is desi...|http://www.apple.com|\n",
      "|               REITs|American Assets T...|http://www.americ...|\n",
      "|     Metals & Mining|Almaden Minerals ...|http://www.almade...|\n",
      "|Transportation & ...|Atlas Air Worldwi...|http://www.atlasa...|\n",
      "|                null|The investment se...|                null|\n",
      "|               India|           Indonesia|            Malaysia|\n",
      "| Aerospace & Defense|Axon Enterprise I...|https://www.axon.com|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "companies.select(\"industry\", \"description\", \"website\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7310"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184071"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(alldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['position', 'company', 'description', 'reviews', 'location']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the Description column in alldata to alldataDesc \n",
    "alldata = alldata.withColumnRenamed('description', 'alldataDesc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['position', 'company', 'alldataDesc', 'reviews', 'location']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|            position|             company|         alldataDesc|             reviews|            location|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Development Director|             ALS TDI|Development Director|                null|                null|\n",
      "|ALS Therapy Devel...| the Development ...| generating aware...| prospects and do...|                 GA.|\n",
      "|       Requirements:|                null|                null|                null|                null|\n",
      "|Bachelor's Degree...| written and pres...| as well as the a...|         spreadsheet|            database|\n",
      "|About ALS Therapy...|                null|                null|                null|                null|\n",
      "|The ALS Therapy D...| the charity unde...|  based in Cambridge|                  MA| has served as on...|\n",
      "|            To Apply|                null|                null|                null|                null|\n",
      "|Please apply at h...| salary requireme...|                null|                null|                null|\n",
      "|ALS TDI is an equ...|                null|  Atlanta, GA 30301 |                null|                null|\n",
      "|An Ostentatiously...|  The Hexagon Lavish|     Job Description|                null|                null|\n",
      "|\"\"The road that l...|                null|                null|                null|                null|\n",
      "|Ostentatious is a...| vigor and the ea...|                null|                null|                null|\n",
      "|      With that said|             Atlanta| Georgia-based sc...|     Hexagon LavishÂ®| has an open posi...|\n",
      "|This position inc...| technical and lo...| the Excitable Pr...|                null|                null|\n",
      "|The Excitable Pri...| setting-up work ...|                null|                null|                null|\n",
      "|*** A solid backg...|                null|                null|                null|                null|\n",
      "|Integrating data ...|                null|                null|                null|                null|\n",
      "|Under general dir...| developing innov...|                null|                null|                null|\n",
      "|Providing support...|                null|                null|                null|                null|\n",
      "|Lead and contribu...|                null|                null|                null|                null|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alldata.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aliasing the dataframe\n",
    "ta = companies.alias('ta')\n",
    "tb = alldata.alias('tb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joining two datasets (Inner join). \n",
    "#Filter out non-US companies from the other companies list\n",
    "\n",
    "joint_table = ta.join(tb, tb['company'] == ta['company name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ticker',\n",
       " 'company name',\n",
       " 'short name',\n",
       " 'industry',\n",
       " 'description',\n",
       " 'website',\n",
       " 'logo',\n",
       " 'ceo',\n",
       " 'exchange',\n",
       " 'market cap',\n",
       " 'sector',\n",
       " 'tag 1',\n",
       " 'tag 2',\n",
       " 'tag 3',\n",
       " 'position',\n",
       " 'company',\n",
       " 'alldataDesc',\n",
       " 'reviews',\n",
       " 'location']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+--------------------+--------------------+---------+\n",
      "|            industry|             website|        company|         description|            position| exchange|\n",
      "+--------------------+--------------------+---------------+--------------------+--------------------+---------+\n",
      "|http://www.invesc...|                null|         design|             PSJ.png|Experience follow...|     null|\n",
      "| sale or distribu...|             PJP.png|    development|http://www.invesc...|Facilitates the d...|476136000|\n",
      "| services and pro...|http://www.invesc...|    development| etc. It is non-d...|Facilitates the d...|NYSE Arca|\n",
      "| they provide inv...|            SGOL.png| transportation|http://www.etfsec...|              Travel|886515000|\n",
      "| sale or distribu...|             PJP.png|    development|http://www.invesc...|Apply health scie...|476136000|\n",
      "| services and pro...|http://www.invesc...|    development| etc. It is non-d...|Apply health scie...|NYSE Arca|\n",
      "| they provide inv...|            SGOL.png| transportation|http://www.etfsec...|              Travel|886515000|\n",
      "| they provide inv...|            SGOL.png| transportation|http://www.etfsec...|              Travel|886515000|\n",
      "|http://www.invesc...|                null|         design|             PSJ.png|he Enterprise Dat...|     null|\n",
      "| they provide inv...|            SGOL.png| transportation|http://www.etfsec...|              Travel|886515000|\n",
      "+--------------------+--------------------+---------------+--------------------+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#showing info from both pages\n",
    "joint_table.select(\"industry\", \"website\", 'company', 'ta.description', \"tb.position\", \"ta.exchange\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_table = joint_table.filter(ta.industry.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_table = joint_table.filter(ta.description.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_table = joint_table.filter(tb.location.isNotNull())\n",
    "joint_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ticker',\n",
       " 'company name',\n",
       " 'short name',\n",
       " 'industry',\n",
       " 'description',\n",
       " 'website',\n",
       " 'logo',\n",
       " 'ceo',\n",
       " 'exchange',\n",
       " 'market cap',\n",
       " 'sector',\n",
       " 'tag 1',\n",
       " 'tag 2',\n",
       " 'tag 3',\n",
       " 'position',\n",
       " 'company',\n",
       " 'alldataDesc',\n",
       " 'reviews',\n",
       " 'location']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            location|                city|\n",
      "+--------------------+--------------------+\n",
      "| and risk factors...| and risk factors...|\n",
      "| and risk factors...| and risk factors...|\n",
      "| implement and/or...| implement and/or...|\n",
      "| and deployment o...| and deployment o...|\n",
      "| and deployment o...| and deployment o...|\n",
      "| re-designing cod...| re-designing cod...|\n",
      "| re-designing inf...| re-designing inf...|\n",
      "|        publications|        publications|\n",
      "| and water sector...| and water sector...|\n",
      "| and Use public t...| and Use public t...|\n",
      "| land use &amp; m...| land use &amp; m...|\n",
      "| FPGAs); leveragi...| FPGAs); leveragi...|\n",
      "| and senior manag...| and senior manag...|\n",
      "| FPGAs); leveragi...| FPGAs); leveragi...|\n",
      "| and support deci...| and support deci...|\n",
      "| Accounts Receiva...| Accounts Receiva...|\n",
      "| Accounts Receiva...| Accounts Receiva...|\n",
      "| Accounts Receiva...| Accounts Receiva...|\n",
      "| re-designing inf...| re-designing inf...|\n",
      "| and troubleshoot...| and troubleshoot...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joint_table.select('location', F.split(joint_table['location'], ',')[0].alias('city')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ngram function to generate unigram and bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import NGram, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the tokennizer\n",
    "tokens = Tokenizer(inputCol = 'description', outputCol = 'token')\n",
    "joint_table = tokens.transform(joint_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = NGram(n=1, inputCol = 'tokens', outputCol = 'ngram')\n",
    "joint_table = ngrams.transform(joint_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              ngrams|         description|\n",
      "+--------------------+--------------------+\n",
      "|[http://www.inves...|http://www.invesc...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joint_table.select(['ngrams', 'description']).limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         description|            unigrams|\n",
      "+--------------------+--------------------+\n",
      "|http://www.invesc...|http://www.invesc...|\n",
      "| etc. It is non-d...|                    |\n",
      "| etc. It is non-d...|                etc.|\n",
      "| etc. It is non-d...|                  it|\n",
      "| etc. It is non-d...|                  is|\n",
      "| etc. It is non-d...|   non-diversified.\"|\n",
      "|             PSJ.png|             psj.png|\n",
      "|http://www.invesc...|http://www.invesc...|\n",
      "| etc. It is non-d...|                    |\n",
      "| etc. It is non-d...|                etc.|\n",
      "| etc. It is non-d...|                  it|\n",
      "| etc. It is non-d...|                  is|\n",
      "| etc. It is non-d...|   non-diversified.\"|\n",
      "|             PSJ.png|             psj.png|\n",
      "|             PSJ.png|             psj.png|\n",
      "|             PSJ.png|             psj.png|\n",
      "|            SILJ.png|            silj.png|\n",
      "|http://www.etfsec...|http://www.etfsec...|\n",
      "|             PSJ.png|             psj.png|\n",
      "|             PSJ.png|             psj.png|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "description = joint_table.select(['ngrams', 'description']).select(\n",
    "    'description', F.explode('ngrams').alias('unigrams')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = joint_table.select(['ngrams', 'description']).select(\n",
    "    'description', F.explode('ngrams').alias('unigrams'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "884"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         description|            unigrams|\n",
      "+--------------------+--------------------+\n",
      "|http://www.invesc...|http://www.invesc...|\n",
      "| etc. It is non-d...|                    |\n",
      "| etc. It is non-d...|                etc.|\n",
      "| etc. It is non-d...|                  it|\n",
      "| etc. It is non-d...|                  is|\n",
      "| etc. It is non-d...|   non-diversified.\"|\n",
      "|             PSJ.png|             psj.png|\n",
      "|http://www.invesc...|http://www.invesc...|\n",
      "| etc. It is non-d...|                    |\n",
      "| etc. It is non-d...|                etc.|\n",
      "| etc. It is non-d...|                  it|\n",
      "| etc. It is non-d...|                  is|\n",
      "| etc. It is non-d...|   non-diversified.\"|\n",
      "|             PSJ.png|             psj.png|\n",
      "|             PSJ.png|             psj.png|\n",
      "|             PSJ.png|             psj.png|\n",
      "|            SILJ.png|            silj.png|\n",
      "|http://www.etfsec...|http://www.etfsec...|\n",
      "|             PSJ.png|             psj.png|\n",
      "|             PSJ.png|             psj.png|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "description.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = joint_table.select(\n",
    "    'description', \n",
    "    F.explode('ngrams').alias('ngrams')\n",
    ").groupBy(['desciption', 'ngram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = joint_table.select(['unigrams', 'description']).select(\n",
    "    'description', F.explode('unigrams').alias('unigrams')).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`city`' given input columns: [unigrams, ta.description, unigrams];;\\n'Project ['city, unigrams#1017]\\n+- Generate explode(unigrams#467), false, [unigrams#1017]\\n   +- Project [unigrams#467, description#14]\\n      +- Project [ticker#10, company name#11, short name#12, industry#13, description#14, website#15, logo#16, ceo#17, exchange#18, market cap#19, sector#20, tag 1#21, tag 2#22, tag 3#23, position#48, company#49, alldataDesc#286, reviews#51, location#52, tokens#446, unigrams#467, ngrams#503, token#795, ngram#819, UDF(tokens#446) AS bigrams#852]\\n         +- Project [ticker#10, company name#11, short name#12, industry#13, description#14, website#15, logo#16, ceo#17, exchange#18, market cap#19, sector#20, tag 1#21, tag 2#22, tag 3#23, position#48, company#49, alldataDesc#286, reviews#51, location#52, tokens#446, unigrams#467, ngrams#503, token#795, UDF(tokens#446) AS ngram#819]\\n            +- Project [ticker#10, company name#11, short name#12, industry#13, description#14, website#15, logo#16, ceo#17, exchange#18, market cap#19, sector#20, tag 1#21, tag 2#22, tag 3#23, position#48, company#49, alldataDesc#286, reviews#51, location#52, tokens#446, unigrams#467, ngrams#503, UDF(description#14) AS token#795]\\n               +- Filter isnotnull(location#52)\\n                  +- Project [ticker#10, company name#11, short name#12, industry#13, description#14, website#15, logo#16, ceo#17, exchange#18, market cap#19, sector#20, tag 1#21, tag 2#22, tag 3#23, position#48, company#49, alldataDesc#286, reviews#51, location#52, tokens#446, unigrams#467, UDF(tokens#446) AS ngrams#503]\\n                     +- Project [ticker#10, company name#11, short name#12, industry#13, description#14, website#15, logo#16, ceo#17, exchange#18, market cap#19, sector#20, tag 1#21, tag 2#22, tag 3#23, position#48, company#49, alldataDesc#286, reviews#51, location#52, tokens#446, UDF(tokens#446) AS unigrams#467]\\n                        +- Project [ticker#10, company name#11, short name#12, industry#13, description#14, website#15, logo#16, ceo#17, exchange#18, market cap#19, sector#20, tag 1#21, tag 2#22, tag 3#23, position#48, company#49, alldataDesc#286, reviews#51, location#52, UDF(description#14) AS tokens#446]\\n                           +- Filter isnotnull(location#52)\\n                              +- Filter isnotnull(description#14)\\n                                 +- Filter isnotnull(industry#13)\\n                                    +- Join Inner, (company#49 = company name#11)\\n                                       :- SubqueryAlias `ta`\\n                                       :  +- Relation[ticker#10,company name#11,short name#12,industry#13,description#14,website#15,logo#16,ceo#17,exchange#18,market cap#19,sector#20,tag 1#21,tag 2#22,tag 3#23] csv\\n                                       +- SubqueryAlias `tb`\\n                                          +- Project [position#48, company#49, description#50 AS alldataDesc#286, reviews#51, location#52]\\n                                             +- Relation[position#48,company#49,description#50,reviews#51,location#52] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anacond3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anacond3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1645.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`city`' given input columns: [unigrams, ta.description, unigrams];;\n'Project ['city, unigrams#1017]\n+- Generate explode(unigrams#467), false, [unigrams#1017]\n   +- Project [unigrams#467, description#14]\n      +- Project [ticker#10, company name#11, short name#12, industry#13, description#14, website#15, logo#16, ceo#17, exchange#18, market cap#19, sector#20, tag 1#21, tag 2#22, tag 3#23, position#48, company#49, alldataDesc#286, reviews#51, location#52, tokens#446, unigrams#467, ngrams#503, token#795, ngram#819, UDF(tokens#446) AS bigrams#852]\n         +- Project [ticker#10, company name#11, short name#12, industry#13, description#14, website#15, logo#16, ceo#17, exchange#18, market cap#19, sector#20, tag 1#21, tag 2#22, tag 3#23, position#48, company#49, alldataDesc#286, reviews#51, location#52, tokens#446, unigrams#467, ngrams#503, token#795, UDF(tokens#446) AS ngram#819]\n            +- Project [ticker#10, company name#11, short name#12, industry#13, description#14, website#15, logo#16, ceo#17, exchange#18, market cap#19, sector#20, tag 1#21, tag 2#22, tag 3#23, position#48, company#49, alldataDesc#286, reviews#51, location#52, tokens#446, unigrams#467, ngrams#503, UDF(description#14) AS token#795]\n               +- Filter isnotnull(location#52)\n                  +- Project [ticker#10, company name#11, short name#12, industry#13, description#14, website#15, logo#16, ceo#17, exchange#18, market cap#19, sector#20, tag 1#21, tag 2#22, tag 3#23, position#48, company#49, alldataDesc#286, reviews#51, location#52, tokens#446, unigrams#467, UDF(tokens#446) AS ngrams#503]\n                     +- Project [ticker#10, company name#11, short name#12, industry#13, description#14, website#15, logo#16, ceo#17, exchange#18, market cap#19, sector#20, tag 1#21, tag 2#22, tag 3#23, position#48, company#49, alldataDesc#286, reviews#51, location#52, tokens#446, UDF(tokens#446) AS unigrams#467]\n                        +- Project [ticker#10, company name#11, short name#12, industry#13, description#14, website#15, logo#16, ceo#17, exchange#18, market cap#19, sector#20, tag 1#21, tag 2#22, tag 3#23, position#48, company#49, alldataDesc#286, reviews#51, location#52, UDF(description#14) AS tokens#446]\n                           +- Filter isnotnull(location#52)\n                              +- Filter isnotnull(description#14)\n                                 +- Filter isnotnull(industry#13)\n                                    +- Join Inner, (company#49 = company name#11)\n                                       :- SubqueryAlias `ta`\n                                       :  +- Relation[ticker#10,company name#11,short name#12,industry#13,description#14,website#15,logo#16,ceo#17,exchange#18,market cap#19,sector#20,tag 1#21,tag 2#22,tag 3#23] csv\n                                       +- SubqueryAlias `tb`\n                                          +- Project [position#48, company#49, description#50 AS alldataDesc#286, reviews#51, location#52]\n                                             +- Relation[position#48,company#49,description#50,reviews#51,location#52] csv\n\r\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\r\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\r\n\tat sun.reflect.GeneratedMethodAccessor53.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-126-43dc4a46975c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m joint_table.select(['unigrams', 'description']).select(\n\u001b[1;32m----> 2\u001b[1;33m     'city', F.explode('unigrams').alias('unigrams')).groupBy(['city', 'unigrams', 'count()']).show()\n\u001b[0m",
      "\u001b[1;32m~\\Anacond3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   1319\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Bob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \"\"\"\n\u001b[1;32m-> 1321\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1322\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anacond3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anacond3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \"cannot resolve '`city`' given input columns: [unigrams, ta.description, unigrams];;\\n'Project ['city, unigrams#1017]\\n+- Generate explode(unigrams#467), false, [unigrams#1017]\\n   +- Project [unigrams#467, description#14]\\n      +- Project [ticker#10, company name#11, short name#12, industry#13, description#14, website#15, logo#16, ceo#17, exchange#18, market cap#19, sector#20, tag 1#21, tag 2#22, tag 3#23, position#48, company#49, alldataDesc#286, reviews#51, location#52, tokens#446, unigrams#467, ngrams#503, token#795, ngram#819, UDF(tokens#446) AS bigrams#852]\\n         +- Project [ticker#10, company name#11, short name#12, industry#13, description#14, website#15, logo#16, ceo#17, exchange#18, market cap#19, sector#20, tag 1#21, tag 2#22, tag 3#23, position#48, company#49, alldataDesc#286, reviews#51, location#52, tokens#446, unigrams#467, ngrams#503, token#795, UDF(tokens#446) AS ngram#819]\\n            +- Project [ticker#10, company name#11, short name#12, industry#13, description#14, website#15, logo#16, ceo#17, exchange#18, market cap#19, sector#20, tag 1#21, tag 2#22, tag 3#23, position#48, company#49, alldataDesc#286, reviews#51, location#52, tokens#446, unigrams#467, ngrams#503, UDF(description#14) AS token#795]\\n               +- Filter isnotnull(location#52)\\n                  +- Project [ticker#10, company name#11, short name#12, industry#13, description#14, website#15, logo#16, ceo#17, exchange#18, market cap#19, sector#20, tag 1#21, tag 2#22, tag 3#23, position#48, company#49, alldataDesc#286, reviews#51, location#52, tokens#446, unigrams#467, UDF(tokens#446) AS ngrams#503]\\n                     +- Project [ticker#10, company name#11, short name#12, industry#13, description#14, website#15, logo#16, ceo#17, exchange#18, market cap#19, sector#20, tag 1#21, tag 2#22, tag 3#23, position#48, company#49, alldataDesc#286, reviews#51, location#52, tokens#446, UDF(tokens#446) AS unigrams#467]\\n                        +- Project [ticker#10, company name#11, short name#12, industry#13, description#14, website#15, logo#16, ceo#17, exchange#18, market cap#19, sector#20, tag 1#21, tag 2#22, tag 3#23, position#48, company#49, alldataDesc#286, reviews#51, location#52, UDF(description#14) AS tokens#446]\\n                           +- Filter isnotnull(location#52)\\n                              +- Filter isnotnull(description#14)\\n                                 +- Filter isnotnull(industry#13)\\n                                    +- Join Inner, (company#49 = company name#11)\\n                                       :- SubqueryAlias `ta`\\n                                       :  +- Relation[ticker#10,company name#11,short name#12,industry#13,description#14,website#15,logo#16,ceo#17,exchange#18,market cap#19,sector#20,tag 1#21,tag 2#22,tag 3#23] csv\\n                                       +- SubqueryAlias `tb`\\n                                          +- Project [position#48, company#49, description#50 AS alldataDesc#286, reviews#51, location#52]\\n                                             +- Relation[position#48,company#49,description#50,reviews#51,location#52] csv\\n\""
     ]
    }
   ],
   "source": [
    "joint_table.select(['unigrams', 'description']).select(\n",
    "    'city', F.explode('unigrams').alias('unigrams')).groupBy(['city', 'unigrams', 'count()']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = NGram(n=2, inputCol = 'tokens', outputCol = 'bigrams')\n",
    "joint_table = ngrams.transform(joint_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(bigrams=[])]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_table.select('bigrams').limit(1).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "new2 = joint_table.select('bigrams').limit(1).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = joint_table.select(['bigrams', 'description']).select(\n",
    "    'description', F.explode('bigrams').alias('bigrams'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         description|             bigrams|\n",
      "+--------------------+--------------------+\n",
      "| etc. It is non-d...|                etc.|\n",
      "| etc. It is non-d...|             etc. it|\n",
      "| etc. It is non-d...|               it is|\n",
      "| etc. It is non-d...|is non-diversified.\"|\n",
      "| etc. It is non-d...|                etc.|\n",
      "| etc. It is non-d...|             etc. it|\n",
      "| etc. It is non-d...|               it is|\n",
      "| etc. It is non-d...|is non-diversified.\"|\n",
      "| leisure products...|             leisure|\n",
      "| leisure products...|    leisure products|\n",
      "| leisure products...|        products and|\n",
      "| leisure products...|        and services|\n",
      "| perpetual subord...|           perpetual|\n",
      "| perpetual subord...|perpetual subordi...|\n",
      "| perpetual subord...|   subordinated debt|\n",
      "| perpetual subord...|            debt and|\n",
      "| perpetual subord...|         and certain|\n",
      "| perpetual subord...|     certain capital|\n",
      "| perpetual subord...| capital securities.|\n",
      "| perpetual subord...|      securities. it|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "location.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry = joint_table.select(['ngrams', 'industry']).select(\n",
    "    'industry', F.explode('ngrams').alias('ngrams'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ticker',\n",
       " 'company name',\n",
       " 'short name',\n",
       " 'industry',\n",
       " 'description',\n",
       " 'website',\n",
       " 'logo',\n",
       " 'ceo',\n",
       " 'exchange',\n",
       " 'market cap',\n",
       " 'sector',\n",
       " 'tag 1',\n",
       " 'tag 2',\n",
       " 'tag 3',\n",
       " 'position',\n",
       " 'company',\n",
       " 'alldataDesc',\n",
       " 'reviews',\n",
       " 'location',\n",
       " 'tokens',\n",
       " 'unigrams',\n",
       " 'ngrams',\n",
       " 'token',\n",
       " 'ngram',\n",
       " 'bigrams']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location = joint_table.select(['location', 'bigrams'])\n",
    "location.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
